{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Building your own n-gram language model\n",
    "\n",
    "In this lab, you will build your own language model based on n-grams and the Maximum Likelihood Estimation.\n",
    "\n",
    "We'll assume that our text is already \"tokenized\" (split up into words). We'll cover this process in more depth in the next module.\n",
    "\n",
    "As an example, let's work with two sentences from \"[The Disappearance of Lady Frances Carfax](https://en.wikipedia.org/wiki/The_Disappearance_of_Lady_Frances_Carfax)\", a short story written by [Sir Arthur Conan Doyle](https://en.wikipedia.org/wiki/Arthur_Conan_Doyle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens for the sentence \"It shows, my dear Watson, that we are dealing\n",
    "# with an exceptionally astude and dangerous man.\"\n",
    "sample1 = ['It', 'shows', ',', 'my', 'dear', 'Watson', ',', 'that',\n",
    "           'we', 'are', 'dealing', 'with', 'an', 'exceptionally',\n",
    "           'astute', 'and', 'dangerous', 'man', '.']\n",
    "# Tokens for the sentence \"How would Lausanne do, my dear Watson?\"\n",
    "sample2 = ['How', 'would', 'Lausanne', 'do', ',', 'my', 'dear',\n",
    "           'Watson', '?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to write a function that splits the `tokens` sequence\n",
    "into its `n`-grams.\n",
    "\n",
    "For instance, when `tokens=sample1` and `n=3`, your function should\n",
    "return:\n",
    "\n",
    "```python\n",
    "[('It', 'shows', ','),\n",
    " ('shows', ',', 'my'),\n",
    " (',', 'my', 'dear'),\n",
    " ...,\n",
    " ('dangerous', 'man', '.')]\n",
    "```\n",
    " \n",
    "Note: You should return a python [`list`](https://docs.python.org/3/tutorial/datastructures.html#more-on-lists) containing [`tuple`](https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences)s. `tuple`s are immutable sequences, which will be useful later on when you build your language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'shows', ','),\n",
       " ('shows', ',', 'my'),\n",
       " (',', 'my', 'dear'),\n",
       " ('my', 'dear', 'Watson'),\n",
       " ('dear', 'Watson', ','),\n",
       " ('Watson', ',', 'that'),\n",
       " (',', 'that', 'we'),\n",
       " ('that', 'we', 'are'),\n",
       " ('we', 'are', 'dealing'),\n",
       " ('are', 'dealing', 'with'),\n",
       " ('dealing', 'with', 'an'),\n",
       " ('with', 'an', 'exceptionally'),\n",
       " ('an', 'exceptionally', 'astute'),\n",
       " ('exceptionally', 'astute', 'and'),\n",
       " ('astute', 'and', 'dangerous'),\n",
       " ('and', 'dangerous', 'man'),\n",
       " ('dangerous', 'man', '.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "def build_ngrams(tokens: List[str], n: int) -> List[Tuple[str]]:\n",
    "    # your code here\n",
    "    ngrams = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if i + n > len(tokens):\n",
    "            break\n",
    "        ngrams.append(tuple(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "# Example:\n",
    "build_ngrams(sample1, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert len(build_ngrams(sample1, n=3)) == 17\n",
    "assert build_ngrams(sample1, n=3)[0] == ('It', 'shows', ',')\n",
    "assert build_ngrams(sample1, n=3)[10] == ('dealing', 'with', 'an')\n",
    "assert len(build_ngrams(sample1, n=2)) == 18\n",
    "assert build_ngrams(sample1, n=2)[0] == ('It', 'shows')\n",
    "assert build_ngrams(sample1, n=2)[10] == ('dealing', 'with')\n",
    "assert len(build_ngrams(sample2, n=2)) == 8\n",
    "assert build_ngrams(sample2, n=2)[0] == ('How', 'would')\n",
    "assert build_ngrams(sample2, n=2)[7] == ('Watson', '?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the current function, there's no way to know whether an n-gram is at the beginning, middle, or end of the sequence. To overcome this problem, n-gram language models often include special \"beginning-of-string\" (BOS) and \"end-of-string\" (EOS) control tokens.\n",
    "\n",
    "Write a new version of your `build_ngrams` function that includes these control tokens. For instance, when `tokens=sample1` and `n=3`, your new function should return:\n",
    "\n",
    "```python\n",
    "[('<BOS>', '<BOS>', 'It'),\n",
    " ('<BOS>', 'It', 'shows'),\n",
    " ('It', 'shows', ','),\n",
    " ('shows', ',', 'my'),\n",
    " (',', 'my', 'dear'),\n",
    " ...,\n",
    " ('dangerous', 'man', '.'),\n",
    " ('man', '.', '<EOS>'),\n",
    " ('.', '<EOS>', '<EOS>')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<BOS>', '<BOS>', 'It'),\n",
       " ('<BOS>', 'It', 'shows'),\n",
       " ('It', 'shows', ','),\n",
       " ('shows', ',', 'my'),\n",
       " (',', 'my', 'dear'),\n",
       " ('my', 'dear', 'Watson'),\n",
       " ('dear', 'Watson', ','),\n",
       " ('Watson', ',', 'that'),\n",
       " (',', 'that', 'we'),\n",
       " ('that', 'we', 'are'),\n",
       " ('we', 'are', 'dealing'),\n",
       " ('are', 'dealing', 'with'),\n",
       " ('dealing', 'with', 'an'),\n",
       " ('with', 'an', 'exceptionally'),\n",
       " ('an', 'exceptionally', 'astute'),\n",
       " ('exceptionally', 'astute', 'and'),\n",
       " ('astute', 'and', 'dangerous'),\n",
       " ('and', 'dangerous', 'man'),\n",
       " ('dangerous', 'man', '.'),\n",
       " ('man', '.', '<EOS>'),\n",
       " ('.', '<EOS>', '<EOS>')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "def build_ngrams_ctrl(tokens: List[str], n: int) -> List[Tuple[str]]:\n",
    "    tokens = [\"<BOS>\"] * (n-1) + tokens +  [\"<EOS>\"]* (n-1)\n",
    "    ngrams = []\n",
    "    for i, token in enumerate(tokens):    \n",
    "        if i + n > len(tokens):\n",
    "            break\n",
    "        ngrams.append(tuple(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "    # your code here\n",
    "\n",
    "# Example:\n",
    "build_ngrams_ctrl(sample1, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert len(build_ngrams_ctrl(sample1, n=3)) == 21\n",
    "assert build_ngrams_ctrl(sample1, n=3)[0] == ('<BOS>', '<BOS>', 'It')\n",
    "assert build_ngrams_ctrl(sample1, n=3)[10] == ('we', 'are', 'dealing')\n",
    "assert len(build_ngrams_ctrl(sample1, n=2)) == 20\n",
    "assert build_ngrams_ctrl(sample1, n=2)[0] == ('<BOS>', 'It')\n",
    "assert build_ngrams_ctrl(sample1, n=2)[10] == ('are', 'dealing')\n",
    "assert len(build_ngrams_ctrl(sample2, n=2)) == 10\n",
    "assert build_ngrams_ctrl(sample2, n=2)[0] == ('<BOS>', 'How')\n",
    "assert build_ngrams_ctrl(sample2, n=2)[9] == ('?', '<EOS>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you can build n-grams, we have almost everything we need to build an n-gram language model.\n",
    "\n",
    "To compute Maximum Likelihood Estimations, you first need to count the number of times each word follows an n-gram of size `n-1`. You can build this structure as a Python [`dict`](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) that maps the n-grams of size `n-1` to another `dict` that maps the following words to their respective counts.\n",
    "\n",
    "For instance, when `texts=[sample1, sample2]` and `n=3`, your function should return:\n",
    "\n",
    "```python\n",
    "{\n",
    "    ('<BOS>', '<BOS>'): {'It': 1, 'How': 1},\n",
    "    ('<BOS>', 'It'): {'shows': 1},\n",
    "    ('<BOS>', 'How'): {'would': 1},\n",
    "    ...\n",
    "    ('my', 'dear'): {'Watson': 2},\n",
    "    ('dear', 'Watson'): {',': 1, '?': 1},\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<BOS>', '<BOS>'): {'It': 1, 'How': 1},\n",
       " ('<BOS>', 'It'): {'shows': 1},\n",
       " ('It', 'shows'): {',': 1},\n",
       " ('shows', ','): {'my': 1},\n",
       " (',', 'my'): {'dear': 2},\n",
       " ('my', 'dear'): {'Watson': 2},\n",
       " ('dear', 'Watson'): {',': 1, '?': 1},\n",
       " ('Watson', ','): {'that': 1},\n",
       " (',', 'that'): {'we': 1},\n",
       " ('that', 'we'): {'are': 1},\n",
       " ('we', 'are'): {'dealing': 1},\n",
       " ('are', 'dealing'): {'with': 1},\n",
       " ('dealing', 'with'): {'an': 1},\n",
       " ('with', 'an'): {'exceptionally': 1},\n",
       " ('an', 'exceptionally'): {'astute': 1},\n",
       " ('exceptionally', 'astute'): {'and': 1},\n",
       " ('astute', 'and'): {'dangerous': 1},\n",
       " ('and', 'dangerous'): {'man': 1},\n",
       " ('dangerous', 'man'): {'.': 1},\n",
       " ('man', '.'): {'<EOS>': 1},\n",
       " ('.', '<EOS>'): {'<EOS>': 1},\n",
       " ('<BOS>', 'How'): {'would': 1},\n",
       " ('How', 'would'): {'Lausanne': 1},\n",
       " ('would', 'Lausanne'): {'do': 1},\n",
       " ('Lausanne', 'do'): {',': 1},\n",
       " ('do', ','): {'my': 1},\n",
       " ('Watson', '?'): {'<EOS>': 1},\n",
       " ('?', '<EOS>'): {'<EOS>': 1}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "def count_ngrams(texts: List[List[str]], n: int) -> Dict[Tuple[str, ...], Dict[str, int]]:\n",
    "    # your code here\n",
    "    # Be sure to use your build_ngrams_ctrl implementation\n",
    "    ngram_count_dict = dict()\n",
    "    for text in texts:\n",
    "        ngrams = build_ngrams_ctrl(text, n)\n",
    "        for ngram in ngrams:\n",
    "            if ngram[:-1] in ngram_count_dict:\n",
    "                count_dict = ngram_count_dict[ngram[:-1]]\n",
    "                if ngram[-1] in count_dict:\n",
    "                    count_dict[ngram[-1]] +=1 \n",
    "                else:\n",
    "                    count_dict[ngram[-1]] = 1\n",
    "            else:\n",
    "                ngram_count_dict[ngram[:-1]] = {ngram[-1] : 1}\n",
    "    return ngram_count_dict\n",
    "\n",
    "# Example:\n",
    "count_ngrams([sample1, sample2], n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert len(count_ngrams([sample1, sample2], n=3)) == 28\n",
    "assert len(count_ngrams([sample1, sample2], n=3)['<BOS>', '<BOS>']) == 2\n",
    "assert count_ngrams([sample1, sample2], n=3)['<BOS>', '<BOS>']['It'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=3)['<BOS>', '<BOS>']['How'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=3)['my', 'dear']['Watson'] == 2\n",
    "assert len(count_ngrams([sample1, sample2], n=2)) == 24\n",
    "assert len(count_ngrams([sample1, sample2], n=2)['<BOS>',]) == 2\n",
    "assert count_ngrams([sample1, sample2], n=2)['<BOS>',]['It'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=2)['<BOS>',]['How'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=2)['dear',]['Watson'] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're almost there! The last step is to convert the counts into probability estimates.\n",
    "\n",
    "When `texts=[sample1, sample2]` and `n=3`, your function should return:\n",
    "\n",
    "```python\n",
    "{\n",
    "    ('<BOS>', '<BOS>'): {'It': 0.5, 'How': 0.5},\n",
    "    ('<BOS>', 'It'): {'shows': 1.0},\n",
    "    ('<BOS>', 'How'): {'would': 1.0},\n",
    "    ...\n",
    "    ('my', 'dear'): {'Watson': 1.0},\n",
    "    ('dear', 'Watson'): {',': 0.5, '?': 0.5},\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<BOS>', '<BOS>'): {'It': 0.5, 'How': 0.5},\n",
       " ('<BOS>', 'It'): {'shows': 1.0},\n",
       " ('It', 'shows'): {',': 1.0},\n",
       " ('shows', ','): {'my': 1.0},\n",
       " (',', 'my'): {'dear': 1.0},\n",
       " ('my', 'dear'): {'Watson': 1.0},\n",
       " ('dear', 'Watson'): {',': 0.5, '?': 0.5},\n",
       " ('Watson', ','): {'that': 1.0},\n",
       " (',', 'that'): {'we': 1.0},\n",
       " ('that', 'we'): {'are': 1.0},\n",
       " ('we', 'are'): {'dealing': 1.0},\n",
       " ('are', 'dealing'): {'with': 1.0},\n",
       " ('dealing', 'with'): {'an': 1.0},\n",
       " ('with', 'an'): {'exceptionally': 1.0},\n",
       " ('an', 'exceptionally'): {'astute': 1.0},\n",
       " ('exceptionally', 'astute'): {'and': 1.0},\n",
       " ('astute', 'and'): {'dangerous': 1.0},\n",
       " ('and', 'dangerous'): {'man': 1.0},\n",
       " ('dangerous', 'man'): {'.': 1.0},\n",
       " ('man', '.'): {'<EOS>': 1.0},\n",
       " ('.', '<EOS>'): {'<EOS>': 1.0},\n",
       " ('<BOS>', 'How'): {'would': 1.0},\n",
       " ('How', 'would'): {'Lausanne': 1.0},\n",
       " ('would', 'Lausanne'): {'do': 1.0},\n",
       " ('Lausanne', 'do'): {',': 1.0},\n",
       " ('do', ','): {'my': 1.0},\n",
       " ('Watson', '?'): {'<EOS>': 1.0},\n",
       " ('?', '<EOS>'): {'<EOS>': 1.0}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "def build_ngram_model(texts: List[List[str]], n: int) -> Dict[Tuple[str, ...], Dict[str, float]]:\n",
    "    # your code here\n",
    "    # Be sure to use your count_ngrams implementation\n",
    "    ngram_model = dict()\n",
    "    ngram_count_dict = count_ngrams(texts, n)\n",
    "    for ngram, count_mapping in ngram_count_dict.items():\n",
    "        count_value_sum = sum(list(count_mapping.values()))\n",
    "        token_2_probability = dict()\n",
    "        for token, count in count_mapping.items():\n",
    "            token_2_probability[token] = count / count_value_sum\n",
    "        ngram_model[ngram] = token_2_probability\n",
    "    return ngram_model\n",
    "# Example:\n",
    "build_ngram_model([sample1, sample2], n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert build_ngram_model([sample1, sample2], n=3)['<BOS>', '<BOS>']['It'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=3)['<BOS>', '<BOS>']['How'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=3)['my', 'dear']['Watson'] == 1.0\n",
    "assert build_ngram_model([sample1, sample2], n=2)['<BOS>',]['It'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=2)['<BOS>',]['How'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=2)['dear',]['Watson'] == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A language model built from only a few sentences is not very informative. Let's scale up and see what your language model looks like when we train on the complete works of Sir Arthur Conon Doyle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = []\n",
    "with open('arthur-conan-doyle.tok.train.txt', 'rt') as fin:\n",
    "    for line in fin:\n",
    "        full_text.append(list(line.split()))\n",
    "model = build_ngram_model(full_text, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <BOS>\n",
      "\t\"\t0.8073\n",
      "\tThe\t0.0304\n",
      "\tHolmes\t0.0230\n",
      "\tI\t0.0167\n",
      "\tIt\t0.0141\n",
      "\t[206 more...]\n",
      "<BOS> It\n",
      "\twas\t0.8235\n",
      "\tis\t0.0515\n",
      "\tmay\t0.0221\n",
      "\thad\t0.0147\n",
      "\tseemed\t0.0074\n",
      "\t[11 more...]\n",
      "It was\n",
      "\ta\t0.1888\n",
      "\tthe\t0.0686\n",
      "\tnot\t0.0562\n",
      "\tonly\t0.0312\n",
      "\tan\t0.0250\n",
      "\t[184 more...]\n",
      "my dear\n",
      "\tWatson\t0.5612\n",
      "\tfellow\t0.1429\n",
      "\tsir\t0.0918\n",
      "\tyoung\t0.0510\n",
      "\tVon\t0.0204\n",
      "\t[12 more...]\n"
     ]
    }
   ],
   "source": [
    "for prefix in [(BOS, BOS), (BOS, 'It'), ('It', 'was'), ('my', 'dear')]:\n",
    "    print(*prefix)\n",
    "    sorted_probs = sorted(model[prefix].items(), key=lambda x: -x[1])\n",
    "    for k, v in sorted_probs[:5]:\n",
    "        print(f'\\t{k}\\t{v:.4f}')\n",
    "    print(f'\\t[{len(sorted_probs)-5} more...]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do these probabilities look reasonable? How can we systematically evaluate the quality of our model? We'll cover this in the next lesson!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Extra:\n",
    " - Smoothing techniques, such as [Laplace Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing), are often added to n-gram language models to deal with probabilities of 0. How might you modify your code to include smoothing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
